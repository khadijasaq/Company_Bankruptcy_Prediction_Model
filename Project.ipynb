{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "655b7c2c",
   "metadata": {},
   "source": [
    "Random Forest Classification with Python and Scikit-Learn\n",
    "\n",
    "In this project, a Random Forest Classifier is built to predict company bankruptcy. Two models are developed: one with default parameters and another with tuned hyperparameters. The implementation is done with Python and Scikit-Learn, utilizing a custom dataset (data.csv) prepared from financial metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d26ad43",
   "metadata": {},
   "source": [
    "Table of Contents\n",
    "\n",
    "Introduction to Random Forest Algorithm\n",
    "\n",
    "Random Forest Algorithm Intuition\n",
    "\n",
    "Ensemble Learning and Feature Importance\n",
    "\n",
    "Problem Statement\n",
    "\n",
    "Dataset Description\n",
    "\n",
    "Import Libraries\n",
    "\n",
    "Import Dataset\n",
    "\n",
    "Exploratory Data Analysis\n",
    "\n",
    "Data Preprocessing\n",
    "\n",
    "Split Data into Separate Training and Test Set\n",
    "\n",
    "\n",
    "Random Forest Classifier with Default Parameters\n",
    "\n",
    "Confusion Matrix\n",
    "\n",
    "Classification Report\n",
    "\n",
    "Results and Conclusion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abbdaad",
   "metadata": {},
   "source": [
    "1. Introduction to Random Forest Algorithm\n",
    "\n",
    "The Random Forest algorithm is a popular ensemble machine learning technique that extends decision trees for improved accuracy and robustness. It belongs to the supervised learning category and is widely used for classification tasks, such as predicting company bankruptcy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bd6b5f",
   "metadata": {},
   "source": [
    "2. Random Forest Algorithm Intuition\n",
    "\n",
    "Random Forest operates by constructing multiple decision trees during training and combining their outputs through majority voting (for classification). Each tree is trained on a random subset of the data and features, reducing overfitting and enhancing generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692f058b",
   "metadata": {},
   "source": [
    "3. Ensemble Learning and Feature Importance\n",
    "\n",
    "Random Forest leverages ensemble learning by aggregating predictions from multiple trees. It also provides feature importance scores, indicating which features (e.g., financial metrics) most influence the bankruptcy prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94c26ba",
   "metadata": {},
   "source": [
    "4. Problem Statement\n",
    "The goal is to predict whether a company will go bankrupt based on financial metrics. This project builds a Random Forest Classifier using Python and Scikit-Learn, utilizing a custom dataset (data.csv) derived from financial indicators."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425bf754",
   "metadata": {},
   "source": [
    "5. Dataset Description\n",
    "\n",
    "The dataset (data.csv) contains financial metrics such as ROA, Operating Profit Rate, Debt Ratios, and others, used to predict bankruptcy status (Bankrupt?). It was prepared from a collection of company financial data, with structural information removed to focus on key attributes. The target variable is binary (0 for not bankrupt, 1 for bankrupt)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8df89d",
   "metadata": {},
   "source": [
    "6. Import Libraries\n",
    "\n",
    "Import necessary Python libraries for data manipulation, visualization, and modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90304d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "import streamlit as st\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00037fb",
   "metadata": {},
   "source": [
    "6. Import Libraries\n",
    "\n",
    "Import necessary Python libraries for data manipulation, visualization, and modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b68ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv('data.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f41e07",
   "metadata": {},
   "source": [
    "8. Exploratory Data Analysis\n",
    "\n",
    "Perform initial data exploration, including summary statistics, missing value checks, and visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b74e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploratory Data Analysis (EDA)\n",
    "eda_results = {}\n",
    "\n",
    "# 1. Summary statistics (mean, median, mode, etc.)\n",
    "eda_results['summary_stats'] = df.describe()\n",
    "eda_results['mode'] = df.mode().iloc[0]\n",
    "\n",
    "# 2. Data types and unique value counts\n",
    "eda_results['data_types'] = df.dtypes\n",
    "eda_results['unique_values'] = df.nunique()\n",
    "\n",
    "# 3. Missing value analysis\n",
    "eda_results['missing_values'] = df.isnull().sum()\n",
    "\n",
    "# 4. Correlation analysis\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(df.corr(), cmap='coolwarm', annot=False)\n",
    "plt.title('Correlation Matrix')\n",
    "plt.savefig('correlation_matrix.png')\n",
    "plt.close()\n",
    "\n",
    "# 5. Distribution of target variable\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.countplot(x='Bankrupt?', data=df)\n",
    "plt.title('Distribution of Bankruptcy Status')\n",
    "plt.savefig('bankruptcy_distribution.png')\n",
    "plt.close()\n",
    "\n",
    "# 6. Percentage of bankrupt vs. non-bankrupt (pie chart)\n",
    "plt.figure(figsize=(6, 6))\n",
    "df['Bankrupt?'].value_counts().plot.pie(autopct='%1.1f%%', labels=['Not Bankrupt', 'Bankrupt'])\n",
    "plt.title('Percentage of Bankruptcy Status')\n",
    "plt.savefig('bankruptcy_pie.png')\n",
    "plt.close()\n",
    "\n",
    "# 7. Feature distribution (top 5 numerical features)\n",
    "numerical_cols = df.select_dtypes(include=np.number).columns[:5]\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i, col in enumerate(numerical_cols, 1):\n",
    "    plt.subplot(2, 3, i)\n",
    "    sns.histplot(df[col], kde=True)\n",
    "    plt.title(f'Distribution of {col}')\n",
    "plt.tight_layout()\n",
    "plt.savefig('feature_distributions.png')\n",
    "plt.close()\n",
    "\n",
    "# 8. Box plots for outlier detection (top 3 features)\n",
    "plt.figure(figsize=(15, 5))\n",
    "for i, col in enumerate(numerical_cols[:3], 1):\n",
    "    plt.subplot(1, 3, i)\n",
    "    sns.boxplot(y=df[col])\n",
    "    plt.title(f'Boxplot of {col}')\n",
    "plt.tight_layout()\n",
    "plt.savefig('boxplots.png')\n",
    "plt.close()\n",
    "\n",
    "# 9. Pairwise relationships (top 4 numerical features)\n",
    "# Use only numerical columns for pairplot axes, with Bankrupt? as hue\n",
    "sns.pairplot(df, vars=numerical_cols[:4], hue='Bankrupt?')\n",
    "plt.savefig('pairplot.png')\n",
    "plt.close()\n",
    "\n",
    "# 10. Feature importance via correlation with target\n",
    "correlations = df.corr()['Bankrupt?'].sort_values(ascending=False)\n",
    "eda_results['feature_correlations'] = correlations\n",
    "\n",
    "# 11. Distribution of top 3 correlated features with target\n",
    "top_correlated = correlations.index[1:4]  # Exclude Bankrupt? itself\n",
    "plt.figure(figsize=(15, 5))\n",
    "for i, col in enumerate(top_correlated, 1):\n",
    "    plt.subplot(1, 3, i)\n",
    "    sns.histplot(df[col], kde=True)\n",
    "    plt.title(f'Distribution of {col} (High Correlation)')\n",
    "plt.tight_layout()\n",
    "plt.savefig('top_correlated_distributions.png')\n",
    "plt.close()\n",
    "\n",
    "# 12. Grouped aggregations by bankruptcy status\n",
    "eda_results['group_by_bankruptcy'] = df.groupby('Bankrupt?')[numerical_cols].mean()\n",
    "\n",
    "# 13. Skewness and kurtosis analysis\n",
    "eda_results['skewness'] = df[numerical_cols].skew()\n",
    "eda_results['kurtosis'] = df[numerical_cols].kurtosis()\n",
    "\n",
    "# 14. Variance analysis\n",
    "eda_results['variance'] = df[numerical_cols].var()\n",
    "\n",
    "# 15. Violin plots for top 3 features by bankruptcy status\n",
    "plt.figure(figsize=(15, 5))\n",
    "for i, col in enumerate(numerical_cols[:3], 1):\n",
    "    plt.subplot(1, 3, i)\n",
    "    sns.violinplot(x='Bankrupt?', y=col, data=df)\n",
    "    plt.title(f'Violin Plot of {col}')\n",
    "plt.tight_layout()\n",
    "plt.savefig('violin_plots.png')\n",
    "plt.close()\n",
    "\n",
    "# Print EDA results\n",
    "print(\"Summary Statistics:\\n\", eda_results['summary_stats'])\n",
    "print(\"\\nMode:\\n\", eda_results['mode'])\n",
    "print(\"\\nData Types:\\n\", eda_results['data_types'])\n",
    "print(\"\\nUnique Values:\\n\", eda_results['unique_values'])\n",
    "print(\"\\nMissing Values:\\n\", eda_results['missing_values'])\n",
    "print(\"\\nFeature Correlations with Bankrupt?:\\n\", eda_results['feature_correlations'])\n",
    "print(\"\\nGroup by Bankruptcy Status:\\n\", eda_results['group_by_bankruptcy'])\n",
    "print(\"\\nSkewness:\\n\", eda_results['skewness'])\n",
    "print(\"\\nKurtosis:\\n\", eda_results['kurtosis'])\n",
    "print(\"\\nVariance:\\n\", eda_results['variance'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c819c756",
   "metadata": {},
   "source": [
    "9. Data Preprocessing and Split Data into Separate Training and Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72cd534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing\n",
    "# Handle missing values (impute with median for numerical)\n",
    "numerical_cols = df.select_dtypes(include=np.number).columns\n",
    "df[numerical_cols] = df[numerical_cols].fillna(df[numerical_cols].median())\n",
    "\n",
    "# Remove outliers using IQR method\n",
    "Q1 = df[numerical_cols].quantile(0.25)\n",
    "Q3 = df[numerical_cols].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "df = df[~((df[numerical_cols] < (Q1 - 1.5 * IQR)) | (df[numerical_cols] > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
    "\n",
    "# No categorical variables, so no encoding needed\n",
    "\n",
    "# Split features and target\n",
    "X = df.drop('Bankrupt?', axis=1)\n",
    "y = df['Bankrupt?']\n",
    "\n",
    "# Scale numerical features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Save scaler for Streamlit app\n",
    "with open('scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "# Print preprocessing results\n",
    "print(\"\\nData preprocessing completed. Shapes:\")\n",
    "print(f\"X_train: {X_train.shape}, X_test: {X_test.shape}, y_train: {y_train.shape}, y_test: {y_test.shape}\")\n",
    "\n",
    "# Save preprocessed data for model training\n",
    "with open('preprocessed_data.pkl', 'wb') as f:\n",
    "    pickle.dump((X_train, X_test, y_train, y_test, X.columns), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa54608b",
   "metadata": {},
   "source": [
    "10. Random Forest Classifier with Default Parameters\n",
    "\n",
    "Train a Random Forest Classifier with default settings and evaluate its performance.\n",
    "\n",
    "11. Confusion Matrix\n",
    "\n",
    "Generate confusion matrices for both models to assess true positives, false positives, true negatives, and false negatives\n",
    "\n",
    "12. Classification Report\n",
    "\n",
    "Produce classification reports for both models, including precision, recall, and F1-score for bankruptcy prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1539c9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessed data\n",
    "with open('preprocessed_data.pkl', 'rb') as f:\n",
    "    X_train, X_test, y_train, y_test, feature_names = pickle.load(f)\n",
    "\n",
    "# Convert y_train and y_test to numerical (from categorical)\n",
    "y_train = y_train.astype(int)\n",
    "y_test = y_test.astype(int)\n",
    "\n",
    "# Train Random Forest Classifier\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate model\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "# Save model\n",
    "with open('model.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "\n",
    "# Print evaluation results\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")\n",
    "print(\"\\nClassification Report:\\n\", report)\n",
    "\n",
    "# Feature importance\n",
    "feature_importance = pd.Series(model.feature_importances_, index=feature_names).sort_values(ascending=False)\n",
    "print(\"\\nTop 10 Feature Importances:\\n\", feature_importance.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd74bdb",
   "metadata": {},
   "source": [
    "Results and Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46252d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# User input for predictions\n",
    "st.subheader(\"Make a Prediction\")\n",
    "input_data = {}\n",
    "for feature in feature_names[:10]:  # Limit to top 10 features for simplicity\n",
    "    input_data[feature] = st.number_input(f\"{feature}\", value=0.0)\n",
    "\n",
    "if st.button(\"Predict\"):\n",
    "    # Prepare input data\n",
    "    input_df = pd.DataFrame([input_data])\n",
    "    input_scaled = scaler.transform(input_df)\n",
    "    prediction = model.predict(input_scaled)\n",
    "    prob = model.predict_proba(input_scaled)[0][1]\n",
    "    \n",
    "    # Display result\n",
    "    result = \"Bankrupt\" if prediction[0] == 1 else \"Not Bankrupt\"\n",
    "    st.write(f\"Prediction: **{result}**\")\n",
    "    st.write(f\"Probability of Bankruptcy: **{prob:.2%}**\")\n",
    "\n",
    "# Conclusion\n",
    "st.header(\"Conclusion\")\n",
    "st.write(\"\"\"\n",
    "Key takeaways:\n",
    "- The dataset is imbalanced, with fewer bankrupt companies, requiring careful model evaluation.\n",
    "- Features like Net Income to Total Assets and Debt Ratios are highly correlated with bankruptcy.\n",
    "- The Random Forest model provides reliable predictions and can be used for real-time decision-making.\n",
    "- Outlier removal and scaling improve model performance.\n",
    "- Future improvements could include handling class imbalance with SMOTE or testing other models.\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
